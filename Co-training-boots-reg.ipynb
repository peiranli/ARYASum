{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import collections\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "MAX_CHARS = 20000\n",
    "def tokenizer(comment):\n",
    "    comment = comment.lower()\n",
    "    comment = re.sub(r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’;#]\", \" \", str(comment))\n",
    "    comment = re.sub(r\"[ ]+\", \" \", comment)\n",
    "    comment = re.sub(r\"\\,+\", \",\", comment)\n",
    "    if (len(comment) > MAX_CHARS):\n",
    "        comment = comment[:MAX_CHARS]\n",
    "    return [x.text for x in nlp.tokenizer(comment) if x.text != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize=tokenizer)\n",
    "RATING = data.Field(sequential=False, use_vocab=False, dtype=torch.float)\n",
    "SENTIMENT = data.LabelField()\n",
    "\n",
    "train_data = data.TabularDataset(path='data/csv/train/boots.csv', format='csv',skip_header=True, fields=[('id',None),('rating',RATING),('text', TEXT), ('sentiment', SENTIMENT)])\n",
    "\n",
    "POLARITY = data.LabelField()\n",
    "\n",
    "test_polarity_data = data.TabularDataset(path='data/csv/test/polarities/boots-tst.csv', format='csv',skip_header=True,fields=[('id',None),('text', TEXT), ('label', POLARITY), ('rating', None)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = torchtext.vocab.Vectors('boots.200d.txt')\n",
    "\n",
    "MAX_VOCAB_SIZE = 40000\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = embedding, \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "SENTIMENT.build_vocab(train_data)\n",
    "\n",
    "POLARITY.build_vocab(test_polarity_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.defaultdict"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./model/sentiment_label_vocab-boots.json', 'w') as outfile:\n",
    "    json.dump(POLARITY.vocab.itos, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/sentiment_text_vocab-boots.json', 'w') as outfile:\n",
    "    json.dump(TEXT.vocab.stoi, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "print(SENTIMENT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {'positive': 0, 'negative': 1})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENTIMENT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTIMENT_stoi = {'positive': 0, 'negative': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "print(POLARITY.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(1)\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_data, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device,\n",
    "    sort=False)\n",
    "\n",
    "test_polarity_iterator = data.BucketIterator(\n",
    "    test_polarity_data, \n",
    "    batch_size = len(test_polarity_data), \n",
    "    device = device,\n",
    "    sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoLabel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, output_dim, pad_idx, seed_words):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        #self.conv = nn.Conv2d(in_channels = 1, out_channels = n_filters, kernel_size = (1, embedding_dim))\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = 1, \n",
    "                                              kernel_size = (1, embedding_dim)) \n",
    "                                    for _ in range(n_filters)\n",
    "                                    ])\n",
    "        \n",
    "        for i in range(len(self.convs)):\n",
    "            self.convs[i].weight = torch.nn.Parameter(seed_words[i].unsqueeze(0))\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        text = text.permute(1, 0)\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        #conved = [F.cosine_similarity(embedded, conv.weight, dim=3) for conv in self.convs] \n",
    "        \n",
    "        #conv_n = [batch size, 1, sent len]\n",
    "        \n",
    "        conved = [conv.permute(0, 2, 1) for conv in conved]\n",
    "            \n",
    "        #conv_n = [batch size, sent len, 1]\n",
    "        \n",
    "        cat = torch.cat(conved,dim=2)\n",
    "        \n",
    "        #conv_n = [batch size, sent len, n_filters]\n",
    "        \n",
    "        weights = F.max_pool1d(cat, cat.shape[2])\n",
    "        \n",
    "        #weights = [batch size, sent len, 1]\n",
    "        \n",
    "        embedded = embedded.squeeze(1)\n",
    "        \n",
    "        scaled_text = torch.mul(embedded, weights)\n",
    "        \n",
    "        #scaled_text = [batch size, sent len, emb dim]\n",
    "        \n",
    "        sen_embedded = torch.mean(scaled_text, dim=1)\n",
    "        \n",
    "        # sen_embedded = [batch size, emb dim]\n",
    "        \n",
    "        sen_embedded = sen_embedded.unsqueeze(1)\n",
    "        \n",
    "        # sen_embedded = [batch size, 1, emb dim]\n",
    "        \n",
    "        sen_embedded = sen_embedded.unsqueeze(1)\n",
    "        \n",
    "        # sen_embedded = [batch size, 1, 1, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(sen_embedded)).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        #conved = [F.cosine_similarity(sen_embedded, conv.weight, dim=3) for conv in self.convs] \n",
    "        \n",
    "        #conv = [batch size, 1, 1]\n",
    "        \n",
    "        cat = torch.cat(conved,dim=2)\n",
    "            \n",
    "        #conv = [batch size, 1, n_filters]\n",
    "        \n",
    "        q = cat.squeeze(1)\n",
    "        \n",
    "        #conv = [batch size, n_filters]\n",
    "        \n",
    "        q = F.softmax(q,dim=1)\n",
    "        \n",
    "        #q = [batch size, output dim]\n",
    "        \n",
    "        \"\"\"h_norm = get_hnorm(q)\n",
    "        \n",
    "        #h_norm = [batch size]\n",
    "        \n",
    "        q_null = F.sigmoid(h_norm).unsqueeze(1)\n",
    "        \n",
    "        #q_null = [batch size, 1]\n",
    "        \n",
    "        q_k = q*(1-q_null)\n",
    "        \n",
    "        q_kplus = torch.cat([q_k[:,:2], q_null, q_k[:, 2:]], dim=1)\n",
    "        \n",
    "        #q_kplus = [batch size, output dim+1]\n",
    "        \n",
    "        fs = torch.sum(q_kplus,dim=0) #[1, output dim+1]\n",
    "        \n",
    "        q2fs = torch.div(torch.mul(q_kplus, q_kplus), fs) #[batch size, output dim]\"\"\"\n",
    "        \n",
    "        fs = torch.sum(q,dim=0) #[1, output dim+1]\n",
    "        \n",
    "        q2fs = torch.div(torch.mul(q, q), fs)\n",
    "        \n",
    "        sum_ = torch.sum(q2fs,dim=1).unsqueeze(1) #[batch size, 1] \n",
    "        \n",
    "        p = torch.div(q2fs, sum_)\n",
    "            \n",
    "        return p, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoPolarity(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, output_dim, pad_idx, seed_words):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        #self.conv = nn.Conv2d(in_channels = 1, out_channels = n_filters, kernel_size = (1, embedding_dim))\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = 1, \n",
    "                                              kernel_size = (1, embedding_dim)) \n",
    "                                    for _ in range(n_filters)\n",
    "                                    ])\n",
    "        \n",
    "        for i in range(len(self.convs)):\n",
    "            self.convs[i].weight = torch.nn.Parameter(seed_words[i].unsqueeze(0))\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        text = text.permute(1, 0)\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        #conved = [F.cosine_similarity(embedded, conv.weight, dim=3) for conv in self.convs] \n",
    "        \n",
    "        #conv_n = [batch size, 1, sent len]\n",
    "        \n",
    "        conved = [conv.permute(0, 2, 1) for conv in conved]\n",
    "            \n",
    "        #conv_n = [batch size, sent len, 1]\n",
    "        \n",
    "        cat = torch.cat(conved,dim=2)\n",
    "        \n",
    "        #conv_n = [batch size, sent len, n_filters]\n",
    "        \n",
    "        weights = F.max_pool1d(cat, cat.shape[2])\n",
    "        \n",
    "        #weights = [batch size, sent len, 1]\n",
    "        \n",
    "        embedded = embedded.squeeze(1)\n",
    "        \n",
    "        scaled_text = torch.mul(embedded, weights)\n",
    "        \n",
    "        #scaled_text = [batch size, sent len, emb dim]\n",
    "        \n",
    "        sen_embedded = torch.mean(scaled_text, dim=1)\n",
    "        \n",
    "        # sen_embedded = [batch size, emb dim]\n",
    "        \n",
    "        sen_embedded = sen_embedded.unsqueeze(1)\n",
    "        \n",
    "        # sen_embedded = [batch size, 1, emb dim]\n",
    "        \n",
    "        sen_embedded = sen_embedded.unsqueeze(1)\n",
    "        \n",
    "        # sen_embedded = [batch size, 1, 1, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(sen_embedded)).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        #conved = [F.cosine_similarity(sen_embedded, conv.weight, dim=3) for conv in self.convs] \n",
    "        \n",
    "        #conv = [batch size, 1, 1]\n",
    "        \n",
    "        cat = torch.cat(conved,dim=2)\n",
    "            \n",
    "        #conv = [batch size, 1, n_filters]\n",
    "        \n",
    "        q = cat.squeeze(1)\n",
    "        \n",
    "        #conv = [batch size, n_filters]\n",
    "        \n",
    "        q = F.softmax(q,dim=1)\n",
    "        \n",
    "        #q = [batch size, output dim]\n",
    "        \n",
    "        \"\"\"h_norm = get_hnorm(q)\n",
    "        \n",
    "        #h_norm = [batch size]\n",
    "        \n",
    "        q_null = F.sigmoid(h_norm).unsqueeze(1)\n",
    "        \n",
    "        #q_null = [batch size, 1]\n",
    "        \n",
    "        q_k = q*(1-q_null)\n",
    "        \n",
    "        q_kplus = torch.cat([q_k[:,:2], q_null, q_k[:, 2:]], dim=1)\n",
    "        \n",
    "        #q_kplus = [batch size, output dim+1]\n",
    "        \n",
    "        fs = torch.sum(q_kplus,dim=0) #[1, output dim+1]\n",
    "        \n",
    "        q2fs = torch.div(torch.mul(q_kplus, q_kplus), fs) #[batch size, output dim]\"\"\"\n",
    "        \n",
    "        fs = torch.sum(q,dim=0) #[1, output dim+1]\n",
    "        \n",
    "        q2fs = torch.div(torch.mul(q, q), fs)\n",
    "        \n",
    "        sum_ = torch.sum(q2fs,dim=1).unsqueeze(1) #[batch size, 1] \n",
    "        \n",
    "        p = torch.div(q2fs, sum_)\n",
    "        \n",
    "        polarity = q[:,0] - q[:,1]\n",
    "            \n",
    "        return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        for param in self.embedding.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        #self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        text = text.permute(1, 0)\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conv_n = [batch size, n_filters, sent len - filter_sizes[n]]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "        \n",
    "        #logits = self.fc(cat)\n",
    "        \n",
    "        #probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        #classes = torch.max(logits, 1)[1]\n",
    "            \n",
    "        #return probs, classes\n",
    "        return cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = CNN(vocab_size, embedding_dim, n_filters, filter_sizes, 1, \n",
    "                 dropout, pad_idx)\n",
    "        \n",
    "        self.fc1 = nn.Linear(len(filter_sizes) * n_filters, 1)\n",
    "        \n",
    "        #self.fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "        #self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        cat = self.cnn(text)\n",
    "        \n",
    "        logits = self.fc1(cat)\n",
    "        \n",
    "        #logits = self.fc2(logits)\n",
    "            \n",
    "        polarity = torch.tanh(logits)\n",
    "        \n",
    "        return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineNet2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = CNN(vocab_size, embedding_dim, n_filters, filter_sizes, 1, \n",
    "                 dropout, pad_idx)\n",
    "        \n",
    "        self.fc1 = nn.Linear(len(filter_sizes) * n_filters, 1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1, output_dim)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        cat = self.cnn(text)\n",
    "        \n",
    "        logits = self.fc1(cat)\n",
    "        \n",
    "        binary = self.fc2(logits)\n",
    "        \n",
    "        probs = F.softmax(binary, dim=1)\n",
    "        \n",
    "        classes = torch.max(binary, 1)[1]\n",
    "            \n",
    "        polarity = torch.tanh(logits)\n",
    "        \n",
    "        return polarity, probs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def train_metric(preds, label):\n",
    "    max_preds = preds.argmax(dim=1)\n",
    "    acc = metrics.accuracy_score(label.cpu().numpy(), max_preds.cpu().numpy())\n",
    "    return acc\n",
    "\n",
    "def train(model, pseudolabel, iterator, optimizer):\n",
    "    \n",
    "    criterion1 = nn.KLDivLoss()\n",
    "    criterion2 = nn.MSELoss()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    pseudolabel.eval()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pols = model(batch.text)  #[batch size, output dim] \n",
    "        \n",
    "        polarity = pseudolabel(batch.text)\n",
    "        \n",
    "        label = batch.rating\n",
    "        \n",
    "        loss1 = criterion2(pols.squeeze(1), polarity.detach())\n",
    "        \n",
    "        loss2 = criterion2(pols.squeeze(1), label)\n",
    "        \n",
    "        #acc = train_metric(probs, label.detach())\n",
    "        \n",
    "        loss = loss1 + loss2\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        #epoch_acc += acc\n",
    "    \n",
    "    return epoch_loss / len(iterator)#, epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, eval_data):\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for e in eval_data.examples:\n",
    "        pol = evaluate_pol(model, e.text)\n",
    "        if pol > 0:\n",
    "            preds.append('positive')\n",
    "        else:\n",
    "            preds.append('negative')\n",
    "        labels.append(e.label)\n",
    "\n",
    "    f1 = metrics.f1_score(labels, preds, average='weighted')\n",
    "    acc = metrics.accuracy_score(labels, preds)\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "def predict(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    if len(sentence) < min_len:\n",
    "        sentence += ['<pad>'] * (min_len - len(sentence))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in sentence]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    pols = model(tensor)\n",
    "    max_preds = preds.argmax(dim = 1)\n",
    "    return max_preds.item()\n",
    "\n",
    "def predict_pol(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok for tok in tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    pols = model(tensor)\n",
    "    return pols.item()\n",
    "\n",
    "def evaluate_pol(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    if len(sentence) < min_len:\n",
    "        sentence += ['<pad>'] * (min_len - len(sentence))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in sentence]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    pols = model(tensor)\n",
    "    return pols.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_pos_seeds = ['nice','good','perfect','great','recommend','well']\n",
    "general_neg_seeds = ['awful','difficult','bad','disappointed','waste','problem']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_pos_seeds = ['nice','good','well','product','just','use']\n",
    "frequent_neg_seeds = ['difficult','broke','awful','sadly','fell','cracked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_pos_seeds = ['highly','love','perfect','great','perfectly','fits','recommend','loves','plenty','room','best']\n",
    "specific_neg_seeds = ['return','however','returning','stars','broke',\"n't\",'disappointed', 'bottom','unfortunately','star','corners']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_seeds = general_pos_seeds + specific_pos_seeds\n",
    "neg_seeds = general_neg_seeds + specific_neg_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('positive', {'nice', 'plenty', 'great', 'best', 'perfectly', 'recommend', 'highly', 'well', 'perfect', 'loves', 'room', 'love', 'fits', 'good'}), ('negative', {'awful', 'disappointed', 'return', 'unfortunately', 'broke', 'however', \"n't\", 'difficult', 'star', 'waste', 'stars', 'returning', 'problem', 'corners', 'bottom', 'bad'})]\n",
      "torch.Size([2, 1, 1, 200])\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "senti_seed_words_d = collections.defaultdict(set)\n",
    "senti_seed_words_d['positive'] = set(pos_seeds)\n",
    "senti_seed_words_d['negative'] = set(neg_seeds)\n",
    "\n",
    "senti_seed_words = sorted(senti_seed_words_d.items(), key=lambda x:SENTIMENT_stoi[x[0]])\n",
    "print(senti_seed_words)\n",
    "\n",
    "SENTI_SEED_WORDS = []\n",
    "for w, lst in senti_seed_words:\n",
    "    temp = []\n",
    "    for e in lst:\n",
    "        temp.append(TEXT.vocab.vectors[TEXT.vocab.stoi[e]].unsqueeze(0))\n",
    "    embeds = torch.cat(temp)\n",
    "    embed = torch.mean(embeds,dim=0)\n",
    "    SENTI_SEED_WORDS.append(embed.unsqueeze(0))\n",
    "SENTI_SEED_WORDS = torch.cat(SENTI_SEED_WORDS)\n",
    "SENTI_SEED_WORDS = SENTI_SEED_WORDS.unsqueeze(1)\n",
    "SENTI_SEED_WORDS = SENTI_SEED_WORDS.unsqueeze(1)\n",
    "print(SENTI_SEED_WORDS.shape)\n",
    "\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "N_FILTERS = 50\n",
    "FILTER_SIZES = [2,3,4]\n",
    "KOUTPUT_DIM = len(POLARITY.vocab)\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "k_senti_model = CombineNet(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, KOUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "k_senti_model = k_senti_model.to(device)\n",
    "\n",
    "k_senti_pseudolabel = PseudoPolarity(INPUT_DIM, EMBEDDING_DIM, KOUTPUT_DIM, KOUTPUT_DIM, PAD_IDX, SENTI_SEED_WORDS)\n",
    "k_senti_pseudolabel.eval()\n",
    "k_senti_pseudolabel = k_senti_pseudolabel.to(device)\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "k_senti_model.cnn.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "k_senti_pseudolabel.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "k_senti_model.cnn.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "k_senti_model.cnn.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "k_senti_pseudolabel.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "k_senti_pseudolabel.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "k_senti_model_optimizer = optim.Adam(filter(lambda p: p.requires_grad, k_senti_model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34138"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "training loss:  0.8957118015281438\n",
      "validation accuracy:  0.6282051282051282\n",
      "validation F1: 0.6084535256410257\n",
      "epoch:  2\n",
      "training loss:  0.6474154520742964\n",
      "validation accuracy:  0.7115384615384616\n",
      "validation F1: 0.7107892107892108\n",
      "epoch:  3\n",
      "training loss:  0.45386832475465516\n",
      "validation accuracy:  0.7564102564102564\n",
      "validation F1: 0.7559285364163413\n",
      "epoch:  4\n",
      "training loss:  0.4268621430046881\n",
      "validation accuracy:  0.7692307692307693\n",
      "validation F1: 0.7664629223849614\n",
      "epoch:  5\n",
      "training loss:  0.4168741622872085\n",
      "validation accuracy:  0.7564102564102564\n",
      "validation F1: 0.7541443053070961\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    print(\"epoch: \",epoch+1)\n",
    "\n",
    "    train_loss = train(k_senti_model, k_senti_pseudolabel, train_iterator, k_senti_model_optimizer)\n",
    "    \n",
    "    print(\"training loss: \",train_loss)\n",
    "    #print(\"training accuracy: \",train_acc)\n",
    "    \n",
    "    valid_acc, valid_f1 = evaluate(k_senti_model, test_polarity_data)\n",
    "    \n",
    "    print(\"validation accuracy: \",valid_acc)\n",
    "    print('validation F1:',valid_f1)\n",
    "    \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7541443053070961\n",
      "0.7564102564102564\n",
      "[[51 26]\n",
      " [12 67]]\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "for e in test_polarity_data.examples:\n",
    "    pol = evaluate_pol(k_senti_model, e.text)\n",
    "    if pol > 0:\n",
    "        preds.append('positive')\n",
    "    else:\n",
    "        preds.append('negative')\n",
    "    labels.append(e.label)\n",
    "\n",
    "\n",
    "#print(metrics.precision_score(labels, preds, average='weighted'))\n",
    "#print(metrics.recall_score(labels, preds, average='weighted'))\n",
    "print(metrics.f1_score(labels, preds, average='weighted'))\n",
    "print(metrics.accuracy_score(labels, preds))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "m = confusion_matrix(labels, preds)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9387118220329285"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_pol(k_senti_model, \"They are cozy warm and comfortable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6828398704528809"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_pol(k_senti_model, \"just like the Fit Flops I wear all summer long .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8613943457603455"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_pol(k_senti_model, \"they would be great for a cruise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2210138887166977"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_pol(k_senti_model, \"and they should be cooler than my Nikes .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20635566115379333"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_pol(k_senti_model, \"I have a blister after a quick two mile walk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(k_senti_model.state_dict(), './model/sentiment-boots.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
